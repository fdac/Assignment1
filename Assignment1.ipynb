{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Written text as operational data\n",
      "\n",
      "Written text is one type of data\n",
      "\n",
      "### Why people write?\n",
      "\n",
      " - To communicate: their thoughts, feelings, urgency, needs, information\n",
      "\n",
      "### Why people communicate?\n",
      "\n",
      "1. To express emotions\n",
      "1. To share information\n",
      "1. To enable or elicit an action\n",
      "1. ...\n",
      "\n",
      "### We will use written text for the purpose other than \n",
      "1. To experience emotion\n",
      "1. To learn something the author intended us to learn\n",
      "1. To do what the author intended us to do\n",
      "\n",
      "### Instead, we will use written text to recognize who wrote it\n",
      " - By calculating and comparing word frequencies in written documents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 1. Dictionaries in python (associative arrays)\n",
      "\n",
      "Plot the frequency distribution of words on a web page."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests, re\n",
      "# re is a module for regular expressions: to detect various combinations of characters\n",
      "import operator\n",
      "\n",
      "# Start from a simple document\n",
      "r = requests .get('http://eecs.utk.edu')\n",
      "\n",
      "# What comes back includes headers and other HTTP stuff, get just the body of the response\n",
      "t = r.text\n",
      "\n",
      "# obtain words by splitting a string using as separator one or more (+) space/like characters (\\s) \n",
      "wds = re.split('\\s+',t)\n",
      "\n",
      "# now populate a dictionary (wf)\n",
      "wf = {}\n",
      "for w in wds:\n",
      "    if w in wf: wf [w] = wf [w] + 1\n",
      "    else:  wf[w] = 1\n",
      "\n",
      "# dictionaries can not be sorted, so lets get a sorted *list*        \n",
      "wfs = sorted (wf .iteritems(), key = operator .itemgetter (1), reverse=True)   \n",
      "\n",
      "# lets just have no more than 15 words \n",
      "ml = min(len(wfs),15)\n",
      "for i in range(1,ml,1):\n",
      "    print wfs[i][0]+\"\\t\"+str(wfs[i][1])   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 2\n",
      "\n",
      "Lots of markup in the output, lets remove it --- \n",
      "\n",
      "use BeautifulSoup and nltk modules and practice some regular expressions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests, re, nltk\n",
      "from BeautifulSoup import BeautifulStoneSoup\n",
      "from nltk import clean_html\n",
      "from collections import Counter\n",
      "import operator\n",
      "\n",
      "# we may not care about the usage of stop words\n",
      "nltk .download ('stopwords')\n",
      "stop_words = nltk.corpus.stopwords.words('english') + [\n",
      " 'ut', '\\'re','.', ',', '--', '\\'s', '?', ')', '(', ':', '\\'',\n",
      " '\\\"', '-', '}', '{', '&', '|', u'\\u2014' ]\n",
      "\n",
      "# We most likely would like to remove html markup\n",
      "def cleanHtml (html):\n",
      "    return BeautifulStoneSoup(clean_html(html),\n",
      "                convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0]\n",
      "\n",
      "# We also want to remove special characters, quotes, etc. from each word\n",
      "def cleanWord (w):\n",
      "    # r in r'[.,\"\\']' stands for regular expression\n",
      "    # any character between the brackets [] is to be removed \n",
      "    return re.sub(r'[.,\"\\.]', \"\", w)\n",
      "       \n",
      "# define a function to get text/clean/calculate frequency\n",
      "def get_wf (URL):\n",
      "    # first get the web page\n",
      "    r = requests .get(URL)\n",
      "    \n",
      "    # Now clean\n",
      "    # remove html markup\n",
      "    t = cleanHtml (r .text) .lower()\n",
      "    \n",
      "    # split string into an array of words using any sequence of spaces \"\\s+\" \n",
      "    wds = re .split('\\s+',t)\n",
      "    \n",
      "    # remove periods, commas, etc stuck to the edges of words\n",
      "    for i in range(len(wds)):\n",
      "        wds [i] = cleanWord (wds [i])\n",
      "    \n",
      "    # If satisfied with results, lets go to the next step: calculate frequencies\n",
      "    # We can write a loop to create a dictionary, but \n",
      "    # there is a special function for everything in python\n",
      "    # in particular for counting frequencies (like function table() in R)\n",
      "    wf = Counter (wds)\n",
      "    \n",
      "    # Remove stop words from the dictionary wf\n",
      "    for k in stop_words:\n",
      "        wf. pop(k, None)\n",
      "        \n",
      "    #how many regular words in the document?\n",
      "    tw = 0\n",
      "    for w in wf:\n",
      "       tw += wf[w] \n",
      "        \n",
      "    \n",
      "    # Get ordered list\n",
      "    wfs = sorted (wf .iteritems(), key = operator.itemgetter(1), reverse=True)\n",
      "    ml = min(len(wfs),15)\n",
      "\n",
      "    #Reverse the list because barh plots items from the bottom\n",
      "    return (wfs [ 1:ml ] [::-1], tw)\n",
      "        \n",
      "# Now populate two lists    \n",
      "(wf_ee, tw_ee) = get_wf('http://eecs.utk.edu')\n",
      "(wf_bu, tw_bu) = get_wf('http://bus.utk.edu')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot the results: are there striking differences in language?\n",
      "import numpy as np\n",
      "import pylab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "def plotTwoLists (wf_ee, wf_bu, title):\n",
      "    f = plt.figure (figsize=(10, 6))\n",
      "    # this is painfully tedious....\n",
      "    f .suptitle (title, fontsize=20)\n",
      "    ax = f.add_subplot(111)\n",
      "    ax .spines ['top'] .set_color ('none')\n",
      "    ax .spines ['bottom'] .set_color ('none')\n",
      "    ax .spines ['left'] .set_color ('none')\n",
      "    ax .spines ['right'] .set_color ('none')\n",
      "    ax .tick_params (labelcolor='w', top='off', bottom='off', left='off', right='off', labelsize=20)\n",
      "\n",
      "    # Create two subplots, this is the first one\n",
      "    ax1 = f .add_subplot (121)\n",
      "    plt .subplots_adjust (wspace=.5)\n",
      "\n",
      "    pos = np .arange (len(wf_ee)+1) \n",
      "    ax1 .tick_params (axis='both', which='major', labelsize=14)\n",
      "    pylab .yticks (pos, [ x [0] for x in wf_ee ])\n",
      "    ax1 .barh (range(len(wf_ee)), [ x [1] for x in wf_ee ], align='center')\n",
      "\n",
      "    ax2 = f .add_subplot (122)\n",
      "    ax2 .tick_params (axis='both', which='major', labelsize=14)\n",
      "    pos = np .arange (len(wf_bu)+1) \n",
      "    pylab .yticks (pos, [ x [0] for x in wf_bu ])\n",
      "    ax2 .barh (range (len(wf_bu)), [ x [1] for x in wf_bu ], align='center')\n",
      "\n",
      "plotTwoLists (wf_ee, wf_bu, 'Most Frequent Words From Two Departments')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Assignment 1\n",
      "\n",
      "1. Compare word frequencies between two works of a single author.\n",
      "1. Compare word frequencies between works of two authors.\n",
      "1. Are there some words preferred by one author but used less frequently by another author?\n",
      "\n",
      "Extra credit\n",
      "\n",
      "1. The frequency of a specific word, e.g., \"would\" should follow a binomial distribution (each regular word in a document is a trial and with probability p that word is \"would\". The estimate for p is N(\"would\")/N(regular word)). Do these binomial distributions for your chosen word differ significantly between books of the same author or between authors? \n",
      "\n",
      "Project Gutenberg is a good source of for fiction and non-fiction.\n",
      "\n",
      "E.g below are two most popular books from Project Gutenberg:\n",
      "- Pride and Prejudice at http://www.gutenberg.org/ebooks/1342.txt.utf-8\n",
      "- Adventures of Huckleberry Finn at http://www.gutenberg.org/ebooks/76.txt.utf-8"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    }
   ],
   "metadata": {}
  }
 ]
}